
\chapwithtoc{Introduction}

Semantic ontologies are important linguistic resources that capture our knowledge of the world. Such lexical resources provide a solid foundation for theoretical linguistic research and become building blocks for other natural language processing research. We work with the multilingual ontology SynSemClass \parencite{uresova-etal-2020-synsemclass} that catalogues semantic classes of word senses. Currently, it only contains verbs.

Manually annotating such ontology is costly. Previously, the SynSemClass project utilized existing lexical resources to build up and extend the ontology. However, rich lexical resources are rare and not harmonized into the same format and a common annotation guideline, thus their usage cannot be easily streamlined. We want to be able to easily extend the ontology for a new language without these resources. In order to do this, we implement a toolchain that creates automatic annotation suggestions. These suggestions will be then given to human annotators who will review them and integrate the correct suggestions into the ontology and discard the erroneous suggestions.

To this end, we use existing tools and evaluate two methods of using these tools. Both of these methods rely on a deep learning classification model for SynSemClass \parencite{SSC_LLM_Suggestions}. The first method, annotation projection, utilizes a sentence-aligned parallel corpus. The method consists of generating annotations on a source language natively supported by the classification model and projecting those automatic annotations onto the target language using automatic word alignment. The second method, zero-shot cross-lingual transfer, relies on the multilingual pretraining of the classification model and generates predictions directly on the target text.

To evaluate the performance of these two methods, we chose Korean as our target language. Korean is an agglutinative head-final East Asian language. This sets it apart from the European head-initial languages that are currently part of the SynSemClass ontology, namely English, Czech, German and Spanish. We build a small Korean--English parallel corpus and manually annotate it for semantic classes and word alignment. We then measure the performance for both methods and verify that the difference is statistically significant.

After discovering that zero-shot cross-lingual transfer performs significantly better, we dive deeper into the reasons why. We find that the loose non-verbatim translation poses a fundamental limit to annotation projection. The text is often rephrased such that the verbs we want to project often change meaning or are eliminated entirely. Furthermore, we show that errors in word alignment contribute to the overall error. We conclude the classification model performs roughly the same on both Korean and English under ideal conditions, but the extra steps of annotation projection introduce cascading errors. 

The last step is creating the final annotation suggestions. Since the classification model creates predictions on single mentions of verbs, our goal is to aggregate the individual predictions into a single suggestion for each class-lemma pair. In \cref{sec:aggregation_experiment}, we investigate what the most efficient aggregation strategy is.