\chapter{Implementation}
\label{implementation}

Our implementation\footnote{\url{https://github.com/petrkasp/synsemclass-pipeline}} is written in Python and uses the methods and their respective libraries as described in \cref{chap:methodology}. For development, we used Visual Studio Code (VS Code). We developed the tool remotely on the Institute of Formal and Applied Linguistics' Artificial Intelligence Cluster.\footnote{\url{https://aic.ufal.mff.cuni.cz/index.php/Main_Page}}

The dependencies are managed by the Python package-management system pip, except for the SynSemClass classification model, which is loaded as a sub-repository.\footnote{Original repository: \url{https://github.com/strakova/synsemclass_ml}} The SynSemClass classification model also has its own dependencies managed by pip. Our code is located in the \texttt{scripts} folder. Here is an overview of the files:

\overfullrule=0pt

\begin{itemize}
    \item \texttt{main.py} --- the entry point of the toolchain. The bulk of the logic is contained here.
    \item \texttt{ko\_filter\_lemma.py} --- contains the definitions of how lemmatization and verb filtering should be done on Korean
    \item \texttt{synsemclass\_prediction.py} --- a wrapper for the SynSemClass classification model for easier use
    \item \texttt{parallel\_corpus\_write.py} --- creates a preannotated evaluation parallel corpus from an unannotated parallel corpus as described in \cref{sec:eval_data}
    \item \texttt{evaluation.py} --- performs the experiments that measure the performance of the toolchain as described in \cref{sec:exp1} using an annotated parallel corpus
    \item \texttt{aggregation\_experiments.py} --- performs the aggregation experiments described in \cref{sec:aggregation_experiment}
    \item \texttt{csv\_manipulator.py} --- makes gold data for the aggregation experiments
    \item \texttt{synsemclass\_writer.py} --- writes the preannotated SynSemClass language-specific file
    \item \texttt{predictions.py} --- a definition of a class used to hold the predictions of the toolchain
    \item \texttt{shared.py} --- utilities shared by multiple scripts
    \item \texttt{tmx\_parser.py} --- parses .tmx parallel corpus files\footnote{We do not use any tmx corpora in our experiments, but we considered several OPUS corpora in this format.}
\end{itemize}

\section{Main toolchain}

\texttt{main.py} takes several arguments as described in the user documentation (\Cref{user_doc}), one of them being the task to perform. Each task follows a similar path:

First we locate verbs with UDPipe 2 \parencite{straka-2018-udpipe}. We use the UDPipe 2 REST service\footnote{\url{https://lindat.mff.cuni.cz/services/udpipe/}} and we perform the requests as in the official UDPipe 2 client\footnote{\url{https://github.com/ufal/udpipe/blob/udpipe-2/udpipe2_client.py}}. At the time we were writing the initial implementation, UDPipe 2 locally was not readily supported and documented. For larger corpora, we send batches of up to 500 sentences. We then use the \texttt{ufal.udpipe} Python package to parse the CoNLL-U output of UDPipe 2. We have a custom \texttt{Verb} class that holds information on each verb. Here, we create instances of this class for each verb found by UDPipe 2. As the toolchain progresses, it fills these instances with more information on the verbs.

The next step is adding alignments with SimAlign \parencite{jalili-sabet-etal-2020-simalign}. We describe SimAlign in \cref{section:simalign}. We give it the words as tokenized by UDPipe 2 and use the itermax matching algorithm --- the custom matching algorithm introduced by \citet{jalili-sabet-etal-2020-simalign} alongside the SimAlign algorithm.

The last common step is adding the predictions from the SynSemClass classification model. For this, we create a \texttt{DeferredPrediction} class that stores the inputs for the predictions inside the \texttt{Verb} instances. We then collect the deferred predictions and run them at once. We do this so that we can run the predictions in larger batches, instead of running each prediction separately.

The results are processed depending on the task chosen. For the parallel corpus creation task, the parallel corpus is written using the native Python library \texttt{xml.etree.cElementTree}. The SynSemClass preannotation file creation task also uses this library. The evaluation task compares the results generated by the toolchain to the gold data and calculates various metrics as described in \cref{sec:exp1}. The task that stores the results for later use in the aggregation experiments described in \cref{sec:aggregation_experiment} uses the native Python libraries \texttt{lzma} and \texttt{pickle}.

\section{Aggregation experiments}

For the aggregation experiments, we load the gold data as created by the \texttt{csv\_manipulator.py} script and the predictions of the toolchain that were created and stored with the \texttt{pred-matrix} task. The aggregation script contains several argument presets for the correct pairings of the gold data and stored predictions. For our manually annotated Korean data, there are two variants --- one filters the gold data to only contain the lemmas that were predicted by the toolchain, as is done with the rest of the languages, and the other variant keeps the gold data intact. The later variant is used in the experiments as described in \cref{sec:aggregation_experiment}.

The \texttt{aggregation\_experiments.py} script then goes through each setting of the aggregation parameters and produces the precision-recall curves for this setting. We use our custom implementation of the computation of the values for the precision-recall curves, as our setting is not strictly classification as described in \cref{subsec:metrics}, so we cannot simply use standard implementations like the one in the Python library \texttt{scikit-learn}. For the plotting, we use the Python library \texttt{matplotlib}.


